[AOSA vol. 2 -- PROCESSING.JS]

THE PROCESSING AND PROCESSING.JS PROJECTS

  Originally developed by Ben Fry and Casey Reas, the Processing programming language started as an open source programming language based on Java to help the electronic arts and visual design communities learn the basics of computer programming in a visual context. Offering a highly simplified model for 2D and 3D graphics compared to most programming language, it quickly became well suited for a wide range of activities, from teaching programming through writing small visualisations to creating multi-wall art installations, and performing a wide variety of tasks, from simply reading in a sequence of strings to acting as the de facto IDE for programming and operating the popular "Arduino" open source hardware prototyping boards. All the while gaining in popularity, Processing has firmly taken its place as an easy to learn, widely used programming language for all things visual, and so much more.

  Processing.js is the sister project of the popular Processing visual programming language, designed to bring Processing to the web without the need for Java or plugins. It started as an attempt by John Resig to see if the Processing language could be ported to the web, using the HTML5 "canvas" element as a graphical context, with a proof of concept library released to the public in 2008. Written with the idea in mind that "your code should just work", Processing.js has been refined over the years to make your data visualizations, digital art, interactive animations, educational graphs, video games, etc. work using web standards and without any plug-ins. You write code using the Processing language, either in the Processing IDE or your favourite editor of choice, include it on a web page using a <canvas> element, and Processing.js does the rest, rendering everything in the canvas element and letting users interact with the graphics in the same way they would with a normal stand-alone Processing sketch.


HOW DOES IT WORK?

  Processing.js is a bit unusual as Open Source project in that the codebase is actually not a codebase, but just a single file, "processing.js", with a big source code object description inside it; the "Processing" object. In terms of how the code is structured, we constantly shuffle things around inside this object as we try to clean it up a little bit with every release. It is actually modelled relatively straightforward, and its function can be described in a single sentence: it rewrites Processing (Java) source code into pure JavaScript source code, and every Processing API function call is mapped to a corresponding function in the Processing object, which effects the same thing on a <canvas> element as the Processing call would effect on a Java Applet canvas.

  For speed, we have two separate code subsets for 2D and 3D functions, and when a sketch is loaded, either one or the other is used for resolving function wrappers so that we don't add bloat to running instances, but in terms of data structures and code flow, if you know JavaScript, you can read Processing.js -- except, perhaps, the parser.

  UNIFYING JAVA AND JAVASCRIPT

    Rewriting Processing source code into JavaScript source code means that you can simply tell the browser to execute the rewritten source, and if you rewrote it correctly, things just work. But making sure the rewrite is correct has taken, and still occasionally takes, quite a bit of effort: Processing syntax is based on Java syntax, which means that the library has to essentially transform Java code into JavaScript code. Initially, this was achieved through iterative replacements of the source code as text string (for those interesting in the early incarnation of the parser, you can find it at https://github.com/jeresig/processing-js/blob/51d280c516c0530cd9e63531076dfa147406e6b2/processing.js#L37 running up to line 266), slowly turning it from a Java string into a JavaScript string. For a small syntax set, this is fine, but as time went on and complexity added on complexity, this approach started to break down, and the parser was completely rewritten to build an Abstract Syntax Tree (AST) instead, first breaking down the Java source code into functional blocks, and then mapping each of those blocks to corresponding JavaScript syntax. The result is that, at the cost of readability (readers are welcome to peruse https://github.com/jeresig/processing-js/blob/v1.3.0/processing.js#L17649 up to line 19217), Processing.js now effectively contains a complete on-the-fly Java-to-JavaScript transcompiler.

    {{ two block comparison: "A Processing sketch and its Processing.js conversion"}}
    @@@
    @@@
    {{ two block comparison: "A Processing sketch and its Processing.js conversion"}}

    This sounds like a great thing, but there are a few problems when converting Java syntax to JavaScript syntax:

    1) Java programs are isolated entities. JavaScript programs share the world with a web page.
    2) Java is strongly typed. JavaScript is not.
    3) Java is a class/instance based OO language. JavaScript is not.
    4) Java has distinct variables and methods. JavaScript does not.
    5) Java allows method overloading. JavaScript does not.
    6) Java allows importing compiled code. JavaScript has no idea what that even means.

    Dealing with these problems has been a tradeoff between what users need, and what we can do given web technologies.

    JAVA MAKES YOU WAIT FOR YOUR PROGRAM TO LOAD THINGS. JAVASCRIPT CAN LOCK UP YOUR ENTIRE BROWSER

      Java programs are isolated entities, running in their own thread in the greater pool of applications on your system. JavaScript programs, on the other hand, live inside a browser, and compete with each other in a way that desktop applications don't. When a Java program loads a file, the program waits until the resource is done loading, and operation resumes as intended. In a setting where the program is an isolated entity on its own, this is fine. The Operating system stays responsive because it's responsible for thread scheduling, and even if the program takes an hour to load all its data, you can still use your computer. On a web page, this is not how things work. If you have a JavaScript "program" waiting for a resource to be done loading, it will lock its process until that resource is available. If you're using a browser that uses one process per tab, it'll lock up your tab, and the rest of the browser is still usable. If you're using a browser that doesn't your entire browser will seem frozen. So, regardless of what the process represents, the page the script runs on won't be usable until the resource is done loading, and it's entirely possible that your JavaScript will lock up the entire browser.

      This is unacceptable on the modern web, where resources are transferred asynchronously, and the page just "does things" while resources are loaded. While this is great for traditioanl web pages, for web applications this is a real brain twister: how do you "do nothing" while a resource loads asynchronously? JavaScript offers the XMLHTTPRequest object, which gets a notification once data is available, but this works because the notification is sent to a dedicated "resume" function. What do you do for dynamic load instructions from arbitrary functions?

      For some things, we decided to force synchronous waiting. Loading a file with strings, for instance, uses a synchronous XMLHTTPRequest, and will halt execution of the page until the data is available. For other things, we had to get creative. Loading images, for instance, uses the browser's buily-in mechanism for loading images - we build a new Image() in JavaScript, set its "src" attribute to the image URL, and the browser does the rest, notifying us that the image is ready through the onload event . This doesn't even rely on an XMLHTTPRequest, but simply exploits the browser's capabilities.

      To make matters easier when you already know which images you are loading, we added preload directives so that the sketch does not start execution until preloading is complete. A user can indicate any number of images to preload via a comment block at the start of the sketch, and Processing.js then track outstanding image loading. The onload event for an image tells us that it  is done transferring and is considered ready to be rendered (rather than simply having been downloaded but not decoded as pixel array in memory yet), after which we can populate the corresponding Processing "PImage" object with the correct values - width, height, pixel data, etc. and clear the image from the list. Once the list is empty, the sketch gets executed, and images used during its lifetime will not require waiting for.
      
      {{preload directives example}}
      @@@

      @@@
      {{preload directives example}}

      For other things, we've had to build more complicated "wait for me" systems. Fonts, unlike images, do not have built in browser loading; at least not as functional as image loading. While it is possible to load a font using a CSS @font-face rule and relying on the browser to make it all happen, there are no events that can notify running code that a font load finished. We are slowly seeing events being added to notify code of a completed download for a font, but even then the browser can need a few to a few hundred more milliseconds to actually then parse the font for use on the page, so acting on that signal will lead to the wrong font being applied. Instead, we embedded a tiny TTF that only conains the letter "A" with impossibly small metrics, and instruct the browser to load this font via an @font-face rule with a data URI that contains the font's bytecode as BASE64 string. This font is so small that we can rely on it being immediately available, and for any other font load instruction we compare text metrics between the desired font, and this tiny font. A hidden div is set up with text styled using the desired font, with our tiny font as fallback. As long as the text in that div is impossibly small, we know the desired font is not available yet, and we simply poll availability this way at set intervals until the text has sensible metrics.

    JAVA IS STRONGLY TYPED. JAVASCRIPT IS NOT.

      In Java, the number 2 and the number 2.0 are different values, and they will do different things during mathematical operations. "1/2" in Java is 0, because the numbers are treated as integers, whereas '1.0/2.0" is 0.5, because the numbers are considered decimal fractions with a non-zero integer and a zero fraction. Mixing integers and floats in Java reverts everything to integers, so "1/2.0" is 0. This lets you write fairly creative math statements in Java, and consequently in Processing, which will generate potentially wildly different results when ported to Processing.js, since JavaScript only knows "numbers". As far as JavaScript is concerned, 2 and 2.0 are the same number, and this can give rise to very interesting bugs when converting a sketch from Processing to JavaScript via Processing.js.

      This might sound like a big issue, and at first we were convinced it would be, too, but you can't argue with real world feedback: it turns out this is almost never an issue for people who port their sketches to the web using Processing.js. Rather than solving this in some cool and creative way, the resolution of this problem was actually remarkably straight-forward; we didn't solve it, and as a design choice, we don't intend to ever revisit that decision. Short of adding a symbol table with strong typing so that we can "fake" types in JavaScript and switch functionality based on type, this incompatibility cannot properly be solved without leaving much harder to find edge case bugs, and so rather than adding bulk to the code and slowdown to execution, we left this quirk in. It is a well-documented quirk, and "good code" won't try to take advantage of Java's implicit number type casting. That said, sometimes you will forget, and the result can be quite interesting.

    JAVA IS CLASS-BASED OBJECT ORIENTED, WITH SEPARATE VARIABLE AND METHOD SPACES. JAVASCRIPT IS NOT.

      JavaScript uses prototype objects, and the inheritance model that comes with it. This means all objects are essentially key/value pairs where each key is a string, and values are either primitives, arrays, objects, or functions. On the inheritance side, prototypes can extend other prototypes, but there is no real concept of "superclass" and "subclass". In order to make "proper" Java-style Object Oriented code work, we had to implement classical inheritance for JavaScript in Processing.js, without making it super slow (we think we succeeded in that respect). We also had to come up with a way to prevent variable names and function names from stepping on each other. Because of the key/value nature of JavaScript objects, defining a variable called "line", followed by a function like "line(x1,y1, x2,y2)" will leave you with an object that uses "whatever was declared last for a key". JavaScript first sets object.line = "some value" for you, and then sets object.line = function(x1,y1,x2,y2) { ...}, overriding what you thought was your variable "line".
            
      It would have slowed down the library a lot to create separate administration for variables and methods/functions, so again the documentation explains that it's a bad idea to use variables and functions with the same name. If everyone wrote "proper" code, this wouldn't be much of a problem, as you want to name variables and functions based on what they're for, or what they do, but the real world does things differently. Sometimes your code won't work, and it's because we decided that your code breaking due to a naming conflict is preferable to your code always working, but always being slow. A second reason for not implementing variable and function separation was that this could break JavaScript code used inside Processing sketches. Closures and the scope chain for JavaScript rely on the key/value nature of objects, so driving a wedge in that by writing our own administration would have also severely impacted performance in terms of JIT compilation and minification.

    JAVA ALLOWS METHOD OVERLOADING. JAVASCRIPT DOES NOT.

      One of Java's more powerful features is that you can define some function, let's say add(int,int), and then define another function with the same name, but a different number of arguments, for instance add(int, int, int), or with different argument types, such as add(ComplexNumber, ComplexNumber). Calling add() with two or three integer arguments will automatically call the right function, and calling add() with floats or Car objects will generate an error. JavaScript, on the other hand, does not support this. In JavaScript, a function is a property, and you can call it as either a variable (in which case JavaScript tells you whether the property exists, reporting its coerced boolean value when it does), or you can call it as a function, using the execution operator (which you will know as parentheses with zero or more arguments between them). If you define a function as add(x,y) and you call it as add(1,2,3,4,5,6), JavaScript is okay with that. It'll set x to 1 and y to 2 and simply ignore the rest. In order to make overloading work, we rewrite functions with the same name but different argument count to a numbered function, so that function(a,b,c) in the source becomes function$3(a,b,c) in the rewritten code, and function(a,b,c,d) becomes function$4(a,b,c,d), ensuring the correct code paths.
      We also mostly solved overloading of functions with same-count, different-type arguments, as long as the argument types are difference, as far as JavaScript can tell. Even if keys are strings in JavaScript, values can tested for their "current type" using a typeof operator, returning either "number", "string", "object" or "fuction" depending on what they represent. Declaring "var x = 3" followed by "x='6'" will cause "typeof x" to report "number" after initial declaration, and "string" after reassignment. As long as functions with the same argument count differ in argument type, we rename them and swith on typeof result. This does not work when the arguments are object, so for these functions we use an additional instanceof check to make function overloading work. In fact, the only place where we cannot successfully transcompile overloaded functions is where the argument count is the same between functions, and their type is numerical. As mentioned, JavaScript only has one numerical type, so if you declare different functions add(int x, int y), add(float x, float y) and add(double x, double y), those will clash. Everything else, however, will work just fine.

    JAVA ALLOWS IMPORTING COMPILED CODE...

      Sometimes, plain Processing is not enough, and additional functionality is introduced in the form of a Processing library. These take the form of a .jarchive with compiled Java code, and offer things like networking, audio, video, hardware interfacing and other exotic functions not covered by Processing itself.

      This is a problem, because compiled Java code is JVM byte code. This has given us many headaches: how do we support library imports without writing a Java byte code decompiler? After about a year of discussions, we settled on what may seem the simplest solution. Rather than trying to also cover Processing libraries, we decided to support the import keyword in sketches, and create a Processing.js Library API, so that library developers can write a JavaScript version of their library (where feasible, given the web's nature) so that if they write a package that is used via "import processing.video", native Processing will pick the .jarchvie, and Processing.js will instead pick processing.video.js, thus ensuring that things "just work". This functionality is slated for Processing.js 1.4, and library imports is the last major feature that is still missing from Processing.js (we currenlty support the import keyword only in the sense that it is removed from the source code before conversion), and will be the last major step towards parity.

  WHY PICK JAVASCRIPT IF IT CAN'T DO JAVA?

    This is not an unreasonable question, and it has multiple answers. The most obvious one is that JavaScript comes with the browser. You don't "install" JavaScript yourself, there's no plugin to download first, it's just there. If you want to port something to the web, you're stuck with JavaScript. Although given the flexibility of JavaScript, "stuck with" is really not doing justice to how powerful the language is. So, one reason to pick JavaScript is "because it's already there". Pretty much every device that is of interest comes with a JavaScript-capable browser these days. The same cannot be said for Java, which is being offered less and less as a preinstalled technology.

    However, the proper answer explains that it's not really true that JavaScript "can't do" the things that Java does; it'd just be slower. Even though out of the box JavaScript can't do some of the things Java does, it's still a proper programming language and it can be made to emulate any other programming language, at the cost of speed. We could, technically, write a full Java interpreter, with a String heap, separate variable and method models, class vs. instance Object Orientation with rigid class hierarchies, and everything else under the Sun (or, these days, Oracle), but that's not what we're in it for: Processing.js is about offering a Processing-to-the-web conversion, in as little code as is necessary for that. This means that even though we decided not to make it do certain Java things, our library has one huge benefit: it can cope with JavaScript really, really well.

    In fact, during a meeting between the Processing.js and Processing people, at Bocoup in Boston, in 2010, Ben Fry asked John Resig why he used regular expression replacement and only partial conversion instead of doing a proper parser and compiler. John's response was that it was important to him that people be able to mix Processing syntax (Java) and JavaScript without having to choose between them. That initial choice has been crucial in shaping the philosophy of Processing.js ever since. We've worked hard to keep it true in our code, and we can see a clear payoff when we look at all the "purely web" users of Processing.js, who never used Processing, and will happily mix Processing and JavaScript syntax with things working just fine.

    {{an example of JavaScript and Processing working together}}
    @@@
      // JavaScript (would throw an error in native Processing)
      var cs = { x: 50,
                 y: 0,
                 label: "my label",
                 rotate: function(theta) {
                           var nx = this.x*cos(theta) - this.y*sin(theta);
                           var ny = this.x*sin(theta) + this.y*cos(theta);
                           this.x = nx; this.y = ny; }};

      // Processing
      float angle = 0;

      void setup() {
        size(200,200);
        strokeWeight(15); }

      void draw() {
        translate(width/2,height/2);
        angle += PI/frameRate;
        while(angle>2*PI) { angle-=2*PI; }
        jQuery('#log').text(angle); // JavaScript (error in native Processing)
        cs.rotate(angle);           // legal JavaScript and Processing!
        stroke(random(255));
        point(cs.x, cs.y); }
    @@@
    {{an example of JavaScript and Processing working together}}

    A lot of things in Java are promises: strong typing is a content promise to the compiler, visibility is a promise on who will call methods and reference variables, interfaces are promises that instances contain the methods the interface describes, etc. Break those promises and the compiler complains. But, if you don't --and this is a one of the most important thoughts for Processing.js-- then you don't need the additional code for those promises in order for a program to work. If you stick a number in a variable, and your code treats that variable as if it has a number in it, then at the end of the day "var varname" is just as good as "int varname". Do you need typing? In Java, you do; in JavaScript, you don't, so why force it in? The same goes for other code promises. If the Processing compiler doesn't complain about your code, then we can strip all the explicit syntax for your promises and it'll still work the same.

    This has made Processing.js a ridiculously useful library for data visualisation, media presentation and even entertainment. Sketches in native Processing work, but sketches that mix Java and JavaScript also work just fine, and sketches that use pure JavaScript, treating Processing.js as a glorified canvas drawing framework also works just fine. In an effort to reach parity with native Processing, without forcing Java-only syntax, the project had been taken in by an audience as wide as the web itself. We've seen activity all over the web using Processing.js. Everyone from IBM to Google has built visualisations and games with Processing.js. People have even won contests writing Processing-on-the-web solutions. Processing.js is making a difference.

    Another great thing about converting Java syntax to JavaScript, while leaving JavaScript untouched, is that we've enabled something we hadn't even thought about ourselves: Processing.js will work with anything that will work with JavaScript. One of the really interesting things that we're now seeing, for instance, is that people are using CoffeeScript (a wonderfully simple, ruby-like programming language that transcompiles to JavaScript) in combination with Processing.js, with really cool results. Even though we set out to build a syntax-aware Processing for the web, people took what we did and used it with brand new syntaxes. They could never have done that if we had made Processing.js simply a Java interpreter. By sticking with code conversion rather than writing a code interpreter, Processing.js has given Processing a reach on the web far beyond what it would have had if it had stayed "Java" only, even if that was Java only in syntax, with execution on the web taken care of by JavaScript. The uptake of our code not just by end users, but also by people who try to integrate it with their own technologies, has been both amazing and inspiring. Clearly we're doing something right, and the web seems happy with what we're doing.


THE CODE COMPONENTS

  Processing.js is presented and developed as a large, single file, but architecturally it represents three different components: 1) the launcher, responsible for converting Processing source to processing.js flavoured JavaScript and executing it, 2) static functionality that can be used by all sketches, and 3) sketch functionality that has to be tied to individual instances.

  THE LAUNCHER

    The launcher component takes care of three things: code preprocessing, code conversion, and sketch execution

    PREPROCESSING

      In the preprocessing step, Processing.js directives are split off from the code, and acted upon. These directives come in two flavours: settings and load instructions. The number is low, keeping with the "it should just work" philosophy, and the only settings that users can effect are related to page interaction. By default a sketch will keep running if the page is not in focus, but the "pauseOnBlur=true" directive sets up a sketch in such a way that it will halt execution when the page the sketch is running on is not in focus, resuming execution when the page is in focus again. Also by default, keyboard input is only routed to a sketch when it is focussed. This is especially important when people run multiple sketches on the same page, as keyboard input intended for one sketch should not be processed by another. However, this functionality can be disabled, routing keyboard events to every sketch that is running on a page, using the "globalKeyEvents=true" directive.

      Load instructions take the form of earlier mentioned image preloading and font preloading. Because images and fonts can be used by multiple sketches, they are loaded and tracked globally, so that different sketches don't incur multiple load attempts for the same resource.

    CODE CONVERSION

      The code conversion consists of decomposing the source code into AST nodes, such as statements and expressions, methods, variables, classes, etc. This AST then expanded to JavaScript source code that builds a sketch-equivalent program when executed. This converted source code makes heavy use of the Processing.js instance framework for setting up class relations, where classes in the Processing source code become JavaScript prototypes with special functions for determining superclasses and bindings for superclass functions and variables.

    EXECUTION

      The final step in the launch process is sketch execution, which consists of determining whether or not all preloading has finished, and if it has, adding the sketch to the list of "running instances" and triggering its JavaScript onLoad event so that any sketch listeners can take the appropriate action. After this the Processing chain is run through: setup(), then draw(), and if the sketch is a looping sketch, setting up an interval call to draw() with an interval length that gets closest to the desired framerate for the sketch.

  STATIC LIBRARY

    Much of Processing.js falls under the "static library" heading, representing constants, universal functions, and universal data types. A lot of these actually do double duty, being defined as global properties, but also getting aliassed by instances for quicker code paths. Global constants such as key codes and color mappings are housed in the Processing object itself, set up once, and then referenced when instances are built via the Processing constructor. The same applies to self-contained helper functions, which lets us keep the code as close to "write once, run anywhere" as we can without sacrificing performance.

    Processing.js has to support a large number of complex data types, not just in order to support the data types used in Processing, but also for its internal workings. These, too, are defined in the Processing constructor:

    - Char, and internal object used to overcome some of the behavioural quirks for Java's "char" datatype.
    - PShape, used to representing shape objects
    - PShapSVG, an extension for PShape objects, built from and representing SVG XML

    For PShapeSVG, we implemented our own SVG-to-canvas-instructions code. Since Processing does not implement full SVG support, the code we saved by not relying on an external SVG library means that we can account for every line of code relating to SVG imports. It only parses what it has to, and doesn't waste space with dead code.

    - XMLElement, an XML document object.
    
    For XMLElement, too, we implemented our own code, relying on the browser to first load the XML element into a Node-based structure, then traveling the node structure to build a leaner object. Again, this means we don't have any dead code sitting in Processing.js, taking up space and potentially causing bugs because a patch accidentally makes use of a function that shouldn't be there.
    
    - PMatrix2D and PMatrix3D, used for matrix operations in 2D and 3D mode
    - PImage, represents an image resource

    This is effectively a wrapper of the Image object, with some additional functions and properties so that its API matches the Processing API.

    - PFont, represents a font resource

    There is no Font object defined for JavaScript (at least for now), so rather than actually storing the font as an object, our PFont implementation loads a font via the browser, computes its metrics based on how the browser renders text with it, and then gets cached as metrics object. For speed, PFonts have a reference to the canvas that was uses to determine the font properties, in case textWidth must be calculated, but because we track PFont objects based on name/size pair, if a sketch uses a lot of distinct text sizes, or fonts in general, this will consume too much memory. As such, PFonts will clear their cached canvas and instead call a generic textWidth computation function when the cache grows too large. As a secondary memory preservation strategy, if the font cache continues to grow after clearing the cached canvas for each PFont, font caching is disabled entirely, and font changes in the sketch simply build new throwaway PFont objects for every change in font name, text size or text leading.

    - DrawingShared, with Drawing2D and Drawing3D, housing all the graphics functions.

    The DrawingShared object is actually the biggest speed trap in Processing.js, responsible for determining which of 2D or 3D modes a sketch is launching in, and then rebinding all graphics functions to either the Drawing2D or Drawing3D object. This ensures short code path for graphics instructions, as 2D Processing sketches cannot used 3D functions, and vice versa. By only binding one of the two sets of graphics functions, we gain speed from not having to switch on graphics mode in every function to determine code path, and we save space by not binding graphics functions that are guaranteed not used.

    - ArrayList, a container that emulates Java's ArrayList
    - HashMap, a container that emulates Java's HashMap

    ArrayList and HashMap in particular are special data structures, because functionality of Java containers relies on the Java concepts of equality and hashing. All objects in Java have an equals() and a hashCode() method.

    For non-hashing containers, objects are resolved based on equality rather than identity. Thus, list.remove(myobject) iterates through the list looking for an element for which element.equals(myobject), rather than element==myobject, is true. Because all objects have an equals method, we implemented a "virtual equals" function on the JavaScript side of things. This function gets two objects as arguments, checks whether either of them implements their own equals() function, and if so, falls through to that function instead. If they don't, and the passed objects are primitives, primitive equality is checked. If they're not, then there is no equality.

    For hashing containers, things are even more interesting, as these act as shortcut trees. The hashing container actually consists of a variable number of real containers, each tied to a specific hashcode. Objects are found based on first finding the container for their hashcode, which is then iterated through using equality evaluation. As all objects in Java have a hashCode method, we also wrote a "virtual hashcode" function, which takes a single object as argument. The function checks whether the object implements hashCode, and if so falls through to that function instead. If it doesn't then the hashcode is computed based on the same hashing algorithm that is used in Java.

    ADMINISTRATION

      As a final property of the static code library, it houses the instance list of all sketches that are currently running on the page. This instance list stores sketches based on the canvas they have been loaded in, so that users can call Processing.getInstanceById('canvasid') and get a reference to their sketch for page interaction purposes.

  INSTANCE CODE

    Instance code takes the form of "p.functor = function(arg, ...)" definitions for the Processing API, and "p.constant = ..." for sketch state variables ("p" being our reference to the sketch being set up). Neither of these are located in dedicated code blocks. Rather, the code is organized based on function, so that instance code relating to PShape operations is defined near the PShape object, and instance code for graphics functions are defined near, or in, the Drawing2D and Drawing3D objects.

    In order to keep things fast, a lot of code that could be written as static code with an instance wrapper is actually implemented as purely instance code. For instance, the lerpColor(c1, c2, ratio) function, which determines the color corresponding to the linear interpolation of two colors, is defined as an instance function. Rather than having p.lerpColor(c1,c2,ratio) acting as a wrapper for some static function Processing.lerpColor(c1,c2,ratio), the fact that nothing else in Processing.js relies on lerpColor means that code execution is faster if we write it as a pure instance function. While this does "bloat" the instance object, most functions for which we insist on an instance function rather than a wrapper to the static library are small. Thus, at the expense of memory we create really fast code paths. While the full Processing object will take up a one time memory slice worth around 5MB when initially set up, the prerequisite code for individual sketches only takes up about 500KB.

BUILDING PROCESSING.JS

  In addition to the code layout and architecture, Processing.js gets worked on quite intensively, which we can only do because our development approach sticks to a few basic rules. As these rules influence the architecture of Processing.js, it's worth having a brief look at them before closing this chapter.

  MAKE IT WORK

    Writing code that works sounds like a tautological premise; you write code, and by the time you're done your code either works, because that's what you set out to do, or it doesn't, and you're not done yet. However, "make it work" comes with a corollary.

    "Make it work. And when you're done, prove it."

    If there is one thing above all other things that has allowed Processing.js to grow at the pace it has, it is the presence of tests. Any ticket that requires touching the code, be it either by writing new code, or rewriting old code, cannot be marked as resolved until there is a unit or reference test that allows others to verify not only that the code works the way it should, but also that it breaks when it should. For most code, this typically involves a unit test - a short bit of code that calls a function and simply tests whether the function returns the correct values, for both legal and illegal function calls. Not only does this allow us to test code contributions, but it also lets us perform regression tests.

    Before any code is accepted and merged into our stable development branch, the modified Processing.js library is validated against an ever-growing battery of unit tests. Especially big fixes and performance tests are prone to pass their own unit test, but may end up breaking parts that worked fine before the rewrite. Having tests for every function in the API, as well as internal functions, means that as Processing.js grows, we don't accidentally break compatibility with previous versions. Barring destructive API changes, if none of the tests failed before a code contribution or modification, none of the tests are allowed to fail with the new code in.

    {{ unit test code?}}
    @@@
    @@@
    {{ unit test code?:}}

    In addition to regular code unit tests, we also have visual reference tests. As Processing.js is a port of a visual programming language, some tests cannot be performed using just unit tests. Testing to see whether an ellipse gets drawn on the correct pixels, or whether a single-pixel-wide vertical line is drawn crisp or smoothed cannot be determined without a visual reference. Because all big browsers implement the canvas element and Canvas2D API with subtle differences, these things can only be tested by running code in a browser and verifying that the resulting sketch looks the same as what native Processing generates. To make life easier for developers, we use an automated test suite for this, where new test cases are run through Processing, generating "what it should look like" data to be used for pixel comparison. This data is then stored as a comment inside the sketch that generated it, forming a test, and these tests are then run by Processing.js on a visual reference test page which executes each test and performs pixel comparisons between "what it should look like" and "what it looks like". If the pixels are off, the test fails, and the user is presented with three images; one showing what it should look like, one showing how Processing.js rendered it, and one showing the difference between the two, marking problem areas as red pixels, and correct areas as white. Much like unit tests, these tests must pass before any code contribution can be accepted.

  {{ref test illustration - ref test page screenshot}}

  MAKE IT FAST

    In an open source project, making things work is only the first step in the life of a function. Once things work, you want to make sure things work fast. Based on the "if you can't measure it, you can't improve it" principle, most functions in Processing.js don't just come with unit or ref tests, but also with performance tests. Small bits of code that simply call a function, without testing the correctness of the function, are run several hundred times in a row, with the time required for them to run recorded on a special performance test web page. This lets us quantify how well (or not!) Processing.js performs in browsers that support HTML5's <canvas> element. Every time an optimization patch passes unit and ref testing, it is run through our performance test page. JavaScript is a curious beast, and beautiful code can, in fact, run several orders of magnitude slower than code that contains the same lines several times over, with inline code rather than function calls. This makes performance testing crucial. We have been able to speed up certain parts of the library by three orders of magnitude simply by discovering hot loops during perf testing, and reducing the number of function calls by inlining code, and by making functions return the moment they know what their return value should be, rather than having only a single return at the very end of the function.

  MAKE IT SMALL

    There are two ways to make code small. Foremost, write compact code. If you're manipulating a variable multiple times, compact it to a single manipulation (if possible). If you access an object variable multiple times, cache it. If you call a function multiple times, cache the result. Return once you have all the information you need, and generally apply all the tricks a code optimiser would apply yourself. JavaScript is a particularly nice language for this, since it comes with an incredible amount of flexibility. For example, rather than using "if((result = functionresult)!==null) { var = result; } else { var = default;}" in JavaScript this becomes "var = functionresult || default".

    There is also another form of small code, and that's in terms of runtime code. Because JavaScript lets you change function bindings on the fly, running code becomes much smaller if you can say "bind the function for line2D to the function call for line()" once you know that a program runs in 2D rather than 3D mode, so that you don't have to perform "if(mode==2D) { line2D() } else { line3D() }" for every function call that might be either in 2D or 3D mode.

    Finally, there is the process of minification. There are a number of good systems available that let you compress your JavaScript code by renaming variables, stripping whitespace, and applying certain code optimisations that are hard to do by hand while still keeping the code readable. Examples of these are the YUI minifier and Google's closure compiler. We use these technologies in Processing.js to offer end users bandwidth convenience - minification after stripping comments can shrink the library by as much as 50%, and taking advantage of modern browser/server interaction for gzipped content, we can offer the entire Processing.js library in gzipped form in 65KB.

  IF ALL ELSE FAILS, TELL PEOPLE

    Not everything that can currently be done in Processing can be done in the browser. Security models prevent certain things like saving files to the hard disk and performing USB or Serial port I/O, and a lack of typing in JavaScript can have unexpected consequences (such as all math being floating point math). Sometimes we're faced with the choice between adding an incredible amount of code to enable an edge case, or mark the ticket as a "wontfix" issue. In such cases, a new ticket gets filed, typically titled "add documentation that explains why ...".

    In order to make sure these things aren't lost, we have documentation online for people who start using Processing.js with a Processing background, and people who start using Processing.js with a JavaScript background, covering the differences between what is expected, and what actually happens. Certain things just deserve special mention, because no matter how much work we put into Processing.js, there are certain things we cannot add without sacrificing usability. A good architecture doesn't just cover the way things are, it also covers why: without that, you'll just end up having the same discussions about what the code looks like and whether it shouldn't be different, every time the team changes.


CONCLUSION

  Processing.js has seen a long track of development, with many contributors, and even though it can trace it roots all the way back to John Resig's first stab at porting Processing to the web, most of the original code has been replaced, sometimes several times over since 2007. Not just John's code, but many other contributions that made sense at the time when Processing.js was still trying to get near functional parity with native Processing have been replaced by better or faster code. That said, we couldn't have gotten Processing.js to where it is today without the help of everyone who has helped build out the library over the years. We want to acknowledge the contributions of everyone who has helped keep Processing.js alive, either in a small way or by sticking with the project over multiple releases. In no particular order of importance, John Resig, Alistair MacDonald, David Humphrey, Corban Brook, Anna Sobiepanek, Andor Salga, Daniel Hodgin, Scott Downe, Yuri Delendik, Mike Kamermans, Chris Lonnen, Mickael Medel, Matthew Lam, Jon Buckley, Dominic Baranski, Elijah Grey, Thomas Saunders, Abel Allison, Andrew Grimo, Donghui Liu, Edward Sin, Alex Londono, Robert O'Rourke, Thanh Dao, Zhibin Huang, John Turner, Tom Brown, Minoo Ziaei, Ricard Marxer, Matt Postill, Tiago Moreira, Jonathan Brodsky, and Roger Sodre were all instrumental in realising this project, and with continued contributions from some, and new contributions by others, we intend to keep improving Processing.js until parity has been achieved. To put it in the words of Shaun McWhinnie (@himself) in one of his tweets: "Sat down to *learn* processing.js last night, then found out it can parse my existing pde sketches #jobdone". If your Processing code "just works", we've done our job. And when it doesn't, do drop by and let us know what still needs fixing. There's no better motivation for an open source project development team than people saying they want to use it.
